{"cells":[{"cell_type":"code","source":["# Import pandas dataframes for transformation\nimport pandas as pd\n# Import pandas schema library for datatype and schema validation\nimport pandas_schema\n# Import schema validation librarires\nfrom pandas_schema import Column\n# Import Custom validation pandas schema libraries\nfrom pandas_schema.validation import CustomElementValidation,CustomSeriesValidation\nimport numpy as np\n#Import decimal Library\nfrom decimal import *\nimport sys\nimport math\nfrom csv import writer\n# contains string manipulation libraries\nimport re\n#import datatime library\nfrom datetime import datetime\n#import pyspark libraries\nfrom pyspark.sql import *\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import lit\nfrom pyspark.sql.functions import max as max_\nfrom pyspark.sql.functions import col\n# import time library to perfrom time related validations.\nimport time"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Import Libraries","showTitle":true,"inputWidgets":{},"nuid":"9dda10cf-7f65-4881-9bf8-e54b3618124c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["spark.conf.set(\n  \"fs.azure.account.key.metadatapocsalesforce\",\n  dbutils.secrets.get(scope=\"metadatapocsalesforce\",key=\"storageaccesskey\"))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Extract Service Principal details from Azure Key Vault","showTitle":true,"inputWidgets":{},"nuid":"62ce4015-b9bf-445b-b8d6-9443bc6146d4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# set the raw mount location\nraw_zone_mnt = \"/mnt/rawzone/\"\n# set the stage mount location\nstage_zone_mnt = \"/mnt/stagezone/\"\n#set errorlog mnt location\nerrorlog_mnt = stage_zone_mnt+\"Temptable/\"\n# set the Qualified Folder location\nqffilepath = stage_zone_mnt + 'Qualified' +'/'\n# set the Rejected Folder location\nrjfilepath = stage_zone_mnt  + 'Rejected' +'/'\n#print(errorlog_mnt)\n#print(rjfilepath)\nerrorlogtable ='pocdwh.ErrorLog'"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Define ADLS Folder Paths","showTitle":true,"inputWidgets":{},"nuid":"fbc89a11-aec1-443c-87af-33dca27b0f38"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\n-- truncate  table error log table before each execution \nTRUNCATE TABLE pocdwh.SAP_ErrorLog"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a4e7792d-f6ea-4e63-80cc-3509dc1546e9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":[],"pivotAggregation":null,"xColumns":[],"yColumns":[]},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[],"aggError":"","aggData":[],"addedWidgets":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["jdbcHostname = \"poc-metadatadriven.database.windows.net\"\njdbcDatabase = \"MetadataDB\"\njdbcUsername=\"vsagala\"\njdbcPassword=\"Pass@123\"\njdbcPort = 1433\njdbcUrl = \"jdbc:sqlserver://{0}:{1};database={2}\".format(jdbcHostname, jdbcPort, jdbcDatabase)\nconnectionProperties = {\n  \"user\" : jdbcUsername,\n  \"password\" : jdbcPassword,\n  \"driver\" : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n}"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Establish Connection to Configuration DB","showTitle":true,"inputWidgets":{},"nuid":"6219175e-931c-4a32-92c2-b1ebba81a379"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#print(qffilepath)\n#dbutils.fs.ls(rjfilepath)\n#dbutils.fs.ls(\"/mnt/material_sap_raw/2020/05/21\")\n#display(dbutils.fs.ls(\"abfss://edna-materialdatadomain@[REDACTED].dfs.core.windows.net/Raw/SAP/2020/05/21/Prod_Master_Material_Attr_05-21-2020_03_04_06.csv\"))\n#filepath = \"/mnt/material_sap_raw/2020/05/21\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"047c9453-0bf8-4b38-8370-53f7a4832c0d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#dbutils.widgets.text(\"sourcetype\", \"\",\"\")\n#source_type = dbutils.widgets.get(\"sourcetype\")\n#df_datatype_details_query = \"(SELECT SAP_DataType as sapdatatype,SPARK_DataType as sparkdatatype,ValidationFunction as validationfn from dbo.Conf_SAPtoSPARKDataTypeMapping where Source = '\" + source_type + \"') conf\"\n#df_datatype_details_spk = spark.read.jdbc(url=jdbcUrl, table=df_datatype_details_query, properties=connectionProperties)\n#df_datatype_details = df_datatype_details_spk.toPandas() \n#display(df_datatype_details)\n\n#dbutils.widgets.removeAll()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1c59a8f6-5cf6-4599-9852-b555dbe7518a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["df_datatype_details_query = \"(SELECT SAP_DataType as sapdatatype,SPARK_DataType as sparkdatatype,ValidationFunction as validationfn from dbo.Conf_SAPtoSPARKDataTypeMapping) conf\"\ndf_datatype_details_spk = spark.read.jdbc(url=jdbcUrl, table=df_datatype_details_query, properties=connectionProperties)\ndf_datatype_details = df_datatype_details_spk.toPandas() \n#display(df_datatype_details)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Create a Datatypemapping dataframe to map SAP datatypes to Spark datatypes.","showTitle":true,"inputWidgets":{},"nuid":"9357d75f-fded-43e8-a88f-219b5297a573"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"df_datatype_details_spk","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"sapdatatype","nullable":true,"type":"string"},{"metadata":{},"name":"sparkdatatype","nullable":true,"type":"string"},{"metadata":{},"name":"validationfn","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def generate_schemafile_df(sfp):\n  schemafilepath = sfp\n  #read the schema file from the ADLS Storage\n  df_schemafile_raw=spark.read.format('csv').options(header='false',inferSchema='true').load(sfp).toPandas()\n  # remove the first 7 rows which contains the header information.\n  #df_schemafile_raw_splt=df_schemafile_raw.drop(df_schemafile_raw.index[[0,1,2,3]])._c0.str.split(\"|\",expand=True)\n  df_schemafile_raw_splt=df_schemafile_raw._c0.str.split(\"|\",expand=True)\n  # rename the columns to the  names defined in the schema file.\n  df_schemafile_raw_splt_ren=df_schemafile_raw_splt.rename(columns={0: \"Column\", 1: \"FieldName\",2: \"Key\",3: \"sapdatatype\",4: \"Length\",5: \"outputlen\",6: \"decimals\"})\n  #merge the schema file dataframe with the datatype mapping dataframe to create a mapping between SAP and SPARK dattypes\n  df_schemafile_datatypes = pd.merge(df_schemafile_raw_splt_ren, df_datatype_details, on='sapdatatype', how='inner').sort_values(by=['Column'])\n  # update the validation function to include an empty_validation check on key columns.\n  df_schemafile_datatypes.loc[df_schemafile_datatypes['Key'] == 'X', 'empty_check'] = '+empty_validation'\n  df_schemafile_datatypes.loc[df_schemafile_datatypes['Key'] != 'X', 'empty_check'] = ''\n  df_schemafile_datatypes['validationfnconcat'] = df_schemafile_datatypes['validationfn'] + df_schemafile_datatypes['empty_check']\n  df_schemafile_datatypes_final=df_schemafile_datatypes.drop(['validationfn'], axis=1).drop(['empty_check'], axis=1).rename(columns={\"validationfnconcat\": \"validationfn\"})\n  # reset the index of the dataframe as the we have deleted 6 rows from the file which contained the file metadata information\n  df_schemafile_datatypes_resetindex = df_schemafile_datatypes_final.reset_index()\n  # transpose the dataframe to get the header information.\n  df_header_trans = df_schemafile_datatypes_resetindex.transpose()\n  # get the Field names from the transposed dataframe\n  df_header= df_header_trans.iloc[[2], :]\n  #create a new attribute indexno.\n  df_schemafile_datatypes_resetindex['indexno']= df_schemafile_datatypes_resetindex.index\n  return(df_schemafile_datatypes_resetindex)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Generate the header from the SAP Schema File","showTitle":true,"inputWidgets":{},"nuid":"49e334d7-110c-4805-b48c-bf66e80f6a47"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def generate_datafile_df(dfp):\n  datafilepath = dfp\n  #Read the csv file from the mount drive\n  df_datafile_raw_splt=spark.read.option(\"delimiter\", \"|\").csv(datafilepath).toPandas()\n  #replace the column names of the dataframe\n  df_datafile_raw_splt = df_datafile_raw_splt.rename(columns=rename_col_fun)\n  #index the rows\n  df_datafile_raw_splt['indexno']= df_datafile_raw_splt.index\n  #Remove all null values and replace it with empty string to maintaing consistency\n  df_datafile_raw_splt.fillna(\"\", inplace = True)\n  #sort by index no\n  df_datafile_raw_splt.sort_values(by=['indexno'], inplace=True)\n  #return(df_datafile_raw_splt_resetindex)\n  return(df_datafile_raw_splt)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Generate Data File","showTitle":true,"inputWidgets":{},"nuid":"974c9639-b77d-404c-8767-b0738118b83e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def rename_col_fun(x):\n    if \"_c\" in x :\n      return int(x.replace(\"_c\", \"\")) # or None\n    return int(x.replace(\"_c\", \"\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"53105355-96e2-4c5b-9397-cc9cce314946"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["  def merge_schemaanddatafiles(sdf,ddf) : \n    df_sf_dpindx = sdf\n    df_df = ddf\n    del df_sf_dpindx['indexno']\n    # transpose the rows to columns \n    df_header_trans = df_sf_dpindx.transpose()\n    # get the Field names from the transposed dataframe\n    df_header= df_header_trans.iloc[[2], :]\n    #Merge the header file with the data file.\n    df_datafile_with_header=pd.concat([df_header,df_df],axis=0)\n    #Grab the first row of the data frame to set it as the header\n    new_header = df_datafile_with_header.iloc[0] \n    #set the first row as the header\n    df_datafile_with_header = df_datafile_with_header[1:] \n    #set the header row as the dataframe header\n    df_datafile_with_header.columns = new_header \n    return(df_datafile_with_header)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Merge Header and Data Dataframes","showTitle":true,"inputWidgets":{},"nuid":"a7b96fbe-eb28-4e37-826b-95f10e34fbfc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def column_count_check(sdf,ddf,flname):\n  #del ddf['index']\n  del ddf['indexno']\n  if (sdf.empty ==False and ddf.empty ==False):\n      columns_cnt = (len(ddf.columns))\n      rows_cnt = len(sdf)\n      if (columns_cnt != rows_cnt):\n          error_desc='Schema file has '+ str(rows_cnt) + ' attributes and data file has '+ str(columns_cnt) + ' attributes.Schema mismatch'\n          dateTime = datetime.now()\n          # if not already created automatically, instantiate Sparkcontext\n          spark = SparkSession.builder.getOrCreate()\n          column_names = ['error_desc', 'error_file', 'error_time','validationtype']\n          vals = [(error_desc,flname,str(dateTime),'SchemaValidation')]\n          error_df = spark.createDataFrame(vals, column_names)\n          #call the error log function\n          write_log(error_df)  \n      else : print('Schema and File structure match')\n  else: print('Schema file or DataFile is empty')    "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Column Counts Validation","showTitle":true,"inputWidgets":{},"nuid":"484731be-bffa-42c0-bccd-3a500041b411"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def check_duplicates(ddf,flname):\n  # identify all the duplicate records using the inbuilt duplicated function and store it to a dataframe\n  df_rem_indxno =ddf.loc[:, ddf.columns != 'indexno']\n  df_duplicate_records = df_rem_indxno[df_rem_indxno.duplicated(keep=False)]\n  # if there are no duplicates return true\n  if (df_duplicate_records.empty == True) :\n      print (\"No Duplicate records\")\n  else: \n      #initiate the error data frame\n      column_names = ['error_desc', 'error_file', 'error_time','validationtype']\n      error_df = pd.DataFrame(columns = column_names)\n      # Get the rownumber of the duplicate record in the file.\n      df_duplicate_records['index'] = df_duplicate_records.index\n      # concatenate the entire duplicate row using pipe seperator, the last column in each row will contain the duplicate record row number\n      df_duplicate_records['concatenate_row'] = df_duplicate_records.loc[:, df_duplicate_records.columns].apply(lambda row: '|'.join(row.values.astype(str)), axis=1)\n      # assign the error_desc and the context information to the error dataframe\n      error_df[\"error_desc\"] = df_duplicate_records[\"concatenate_row\"]\n      error_df[\"error_file\"] = flname\n      dateTime = datetime.now()\n      error_df['error_time'] = str(dateTime)\n      error_df[\"validationtype\"] = 'check_duplicates'\n      #convert to spark dataframe and send the information to the error log table.\n      df_duplicatecheck_validation_errors_spk = spark.createDataFrame(error_df.astype(str))\n      #call the error log function\n      write_log(df_duplicatecheck_validation_errors_spk)  "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Duplicate Records Check","showTitle":true,"inputWidgets":{},"nuid":"ad1c2c61-49fe-4c1b-9ec4-a537d5e55986"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def check_businesskeyduplicates(sdf,ddf,flname):\n  # Check for the primary key fields in the schema file\n  df_pk_keys =  sdf['Key']=='X'\n  #filter only the PK rows in the schema file.\n  df_pk_attributes = sdf[df_pk_keys]\n  #loop the dataframe to get the complete list of pk attributes for the data frame.\n  if (df_pk_attributes.empty == False):\n    lst =[]\n    for index, row in df_pk_attributes.iterrows():\n      #derive the key values and append it to a list\n      col_name = row['indexno']\n      lst.append(col_name)\n      dfObj = pd.DataFrame(ddf, columns=lst)\n      #identify the duplicate records and store it a dataframe\n      df_duplicatebusinesskeysdups = dfObj[dfObj.duplicated(keep=False)]\n      # if there are no duplicates return true\n    if (df_duplicatebusinesskeysdups.empty == True) :\n        print (\"No Duplicate records\")\n    else:\n      column_names = ['error_desc', 'error_file', 'error_time','validationtype']\n      error_df = pd.DataFrame(columns = column_names)\n      # this will give the row number of the duplicate record in the file.\n      df_duplicatebusinesskeysdups['indexno'] = df_duplicatebusinesskeysdups.index\n      #get the index nos of the duplicate rows from the dataframe\n      df_duplicatebusinesskeysdups_final = df_duplicatebusinesskeysdups[['indexno']]\n      #join on the index with the original dataframe to get the full row vales\n      df_duplicatebusinesskeys_duplicate=pd.merge(ddf,df_duplicatebusinesskeysdups_final,on='indexno')\n      # concatenate the entire duplicate row using pipe seperator, the last column in each row will contain the duplicate record row number\n      df_duplicatebusinesskeys_duplicate['concatenate_row'] = df_duplicatebusinesskeys_duplicate.loc[:, df_duplicatebusinesskeys_duplicate.columns].apply(lambda row: '|'.join(row.values.astype(str)), axis=1)\n      # assign the error_desc and the context information to the error dataframe\n      error_df[\"error_desc\"] = df_duplicatebusinesskeys_duplicate[\"concatenate_row\"]\n      error_df[\"error_file\"] = flname\n      dateTime = datetime.now()\n      error_df['error_time'] = str(dateTime)\n      error_df[\"validationtype\"] = 'check_businesskey_duplicates'\n      #convert to spark dataframe and send the information to the error log table.\n      df_duplicatecheck_validation_errors_spk = spark.createDataFrame(error_df.astype(str))\n      #call the error log function\n      write_log(df_duplicatecheck_validation_errors_spk)  \n  else: \n    print (\"No Business keys defined\")\n    # When no keys are defined to validate the uniqueness of the record, check for exact duplicates.\n    check_duplicates(ddf,flname)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Duplicate Records Check - Business Keys","showTitle":true,"inputWidgets":{},"nuid":"512db5f5-a221-4f37-8bac-15f830d7612e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def file_exists(path):\n  try:\n    dbutils.fs.ls(path)\n    return True\n  except Exception as e:\n    if 'java.io.FileNotFoundException' in str(e):\n      return False\n    else:\n      raise"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"File Exists Validation","showTitle":true,"inputWidgets":{},"nuid":"ee252d9c-8461-430d-9672-deb316c881c6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def df_isempty(dfp):\n  datafilepath = dfp\n  #Read the csv file from the mount drive\n  df_datafile_raw=spark.read.format('csv').options(header='false',inferSchema='true').load(datafilepath).toPandas()\n  #print(df_datafile_raw)  \n  if (df_datafile_raw.empty == True):\n    return True\n  else:\n    return False\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Empty Dataframe Validation","showTitle":true,"inputWidgets":{},"nuid":"1760a664-a7bb-42a0-bbf9-7f7ea361c30a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def check_int(num):\n    default = 0\n    try:\n        if num is None or num =='' :\n          int(default)\n        else: \n          int(num)\n    except ValueError:\n        return False\n    return True\n  \ndef check_string(st):\n    try:\n        str(st)\n    except ValueError:\n        return False\n    return True\n  \ndef check_decimal(dec):\n  default = 0\n  try:\n        if dec is None or dec =='':\n          Decimal(default)\n        else:\n          if \",\" in dec :\n            Decimal(dec.replace(',',''))\n          else: Decimal(dec)\n  except InvalidOperation:\n        return False\n  return True\n  \ndef check_float(flt):\n    default =0\n    try:\n        if flt is None or  flt =='':\n          float(default)\n        else :\n          if \",\" in dec :\n            float(flt.replace(',',''))\n          else:float(flt)\n    except InvalidOperation:\n        return False\n    return True\n  \ndef check_missingvalues(ip_val):\n   return False if len(str(ip_val)) == 0  else True\n\ndef validate_date(date_text):\n    dateformat = \"%Y%m%d\"\n    try:\n      if date_text == '' or date_text=='00000000':\n          date_text= '19000101'\n      else:\n        validdate=datetime.strptime(date_text, dateformat)\n    #except InvalidOperation:\n    except ValueError:\n        return False\n    return True\n\ndef validate_time(time_text):\n    timeformat = \"%H%M%S\"\n    try:\n      if time_text == '':\n          time_text= '000000'\n      else:\n        validtime = time.strptime(time_text, timeformat)\n    except ValueError:\n        return False\n    return True"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"DataType Validation Function Definitions","showTitle":true,"inputWidgets":{},"nuid":"5c14c4ad-fb5a-481d-a666-1feb89049e49"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["decimal_validation = [CustomElementValidation(lambda d: check_decimal(d), 'is not decimal')]\nint_validation = [CustomElementValidation(lambda i: check_int(i), 'is not integer')]\nstring_validation = [CustomElementValidation(lambda s: check_string(s), 'is not string')]\nnull_validation = [CustomElementValidation(lambda d: d is not np.nan, 'this field cannot be null')]\nempty_validation = [CustomElementValidation(lambda ev: check_missingvalues(ev), 'this field cannot have empty values')]\ndate_validation = [CustomElementValidation(lambda dt: validate_date(dt), 'Incorrect date format, should be in YYYYMMDD format')]\ntime_validation = [CustomElementValidation(lambda tm: validate_time(tm), 'Incorrect time format, should be HHMMDD format')]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Custom Validation Functions Definition","showTitle":true,"inputWidgets":{},"nuid":"109ef6f5-f036-4d41-a698-5e55a8624e2a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def write_csv(df,qffilepath,flname,fltimestamp):\n  dateTime = datetime.now() \n  ldYear = str(dateTime.year)\n  ldMonth = str(dateTime.month)\n  ldDay = str(dateTime.day)\n  file_name = flname\n  if len(ldMonth) ==1:\n    ldMonth = '0'+ldMonth\n  else : ldMonth\n  if len(ldDay) ==1:\n    ldDay = '0'+ldDay\n  else : ldDay\n  df['Year'] =ldYear\n  df['Month']=ldMonth\n  df['Day'] = ldDay\n  #path = \"/mnt/material_sap_stage/Qualified/Prod_Master_Mat_Sales_Attr/\"\n  df_datafile_with_header_spk = spark.createDataFrame(df.astype(str))\n  df_datafile_with_header_spk_tmpstmp = df_datafile_with_header_spk.withColumn(\"StageZoneTimestamp\", lit(dateTime)).withColumn(\"File_Name\", lit(flname)).withColumn(\"File_Time\", lit(fltimestamp))\n  df_datafile_with_header_spk_tmpstmp.write.format(\"delta\").mode(\"append\").partitionBy('Year','Month','Day').option(\"mergeSchema\", \"true\").save(qffilepath)\n  #display(df_datafile_with_header_spk_tmpstmp)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Write to parquet in the Qualified Folder in Stage Zone","showTitle":true,"inputWidgets":{},"nuid":"33cfdda3-58ac-4e68-a277-d1f26c0516c8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def write_log(error_df):\n  error_df.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(errorlogtable)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Write Error Logs to Delta Table","showTitle":true,"inputWidgets":{},"nuid":"29780856-6ccf-41ef-80c9-596bce061df1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def write_log_todb(error_df):\n  df_error_desc_spk = spark.createDataFrame(error_df.astype(str))\n  df_error_write = DataFrameWriter(df_error_desc_spk) \n  #write audit record to database.\n  df_error_write.jdbc(url=jdbcUrl, table= \"[dbo].[Log_DBX_DataValidations_ErrorDetails]\", mode =\"append\", properties = connectionProperties) "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Write Error Logs to Azure SQL Table","showTitle":true,"inputWidgets":{},"nuid":"7cebcf0d-b612-4fdd-a315-93ff2a04c55b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def validate_datatype(colindx,validation,d,sapdatatype,flname,colname):\n  #set the parameter values\n  col_index = colindx\n  validation_type =validation\n  datatype= sapdatatype\n  col_name = colname\n  df_vald=d\n  #rename the columnname of the dataframe\n  df_vald.rename(columns = {col_index:col_name}, inplace = True) \n  #convert function name to object\n  function=eval(validation_type)\n  #validate the column datatype\n  schema=pandas_schema.column.Column(col_name,function)\n  #capture errors from validation function\n  errors = schema.validate(df_vald[col_name])\n  #initialize the error dataframe\n  error_df = pd.DataFrame({'col':errors})\n  #capture the error rows\n  errors_index_rows = [e.row+1 for e in errors]\n  if error_df.empty == False :\n    error_df = pd.DataFrame({'error_desc':errors})\n    error_df['error_file'] = flname\n    dateTime = datetime.now()\n    error_df['error_time'] = str(dateTime)\n    error_df[\"validationtype\"] = 'check_datatype'\n    #display(error_df)\n    return(error_df)\n  else: print('No Records to print')  "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Validate DataType of the File Attributes","showTitle":true,"inputWidgets":{},"nuid":"1135e2d1-4299-42e5-911c-2645a34cbf63"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def check_float_datatype(flt):\n    default ='0'\n    try:\n        if flt is None or  flt =='':\n          float(default)\n        else :\n          float(flt)\n    except ValueError:\n        return False\n    return True"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Float datatype check for Scale and Precision validation","showTitle":true,"inputWidgets":{},"nuid":"a2e3b7a2-21e6-4a5a-9466-d140da4a5d03"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def precision_and_scale(col_idx,dtlen,dtprec,dtype,ddf,fl,colname):\n  max_len = dtlen\n  col_name = colname\n  error_desc = []\n  errorfile = []\n  errortimestamp=[]\n  validationtype=[]\n  for index, row in ddf.iterrows():\n    field_val=row[col_idx]\n    idx= index\n    if (dtprec>0) :\n      if check_float_datatype(field_val) == True:\n        if (field_val is None) :\n          field_val ='0.00'\n        else:\n          if (len(field_val) )==0 : \n            field_val ='0.00'\n          else : field_val\n        deci = Decimal(field_val.replace(',',''))\n        exp=deci.as_tuple().exponent\n        mag=int(deci)\n        exp_len=abs(exp)\n        mag_len=len(str(mag))\n        if (mag_len >max_len) or (exp_len > dtprec) :\n          error_value= '{row: '+str(idx)+' column:\" '+ col_name +'\"} Value'+ str(deci) +' has exceeded the maximum length limit of Magnitude' + str(max_len) +  ' and/or maximum scale limit of ' + str(dtprec) + ' respectively'\n        else: error_value ='No Error'\n        dateTime = datetime.now()\n        error_desc.append(error_value)\n        errorfile.append(fl)\n        errortimestamp.append(dateTime)\n        validationtype.append(\"checkdatatypelength\")\n      # for the rest of the datatypes where the precision is not required check the field value length against the maximum allowable length  \n    elif (dtprec==0):\n      if field_val is None:\n          field_val =''\n      else : field_val\n      if (len(str(field_val)) > max_len):\n        print(\"entered this loop\")\n        error_value='{row: '+str(idx)+' column:\"'+ col_name +'\"} Value '+ str(field_val)+ '  has the exceeded maximum length limit of ' +str(max_len)\n      else:error_value ='No Error'\n      dateTime = datetime.now()\n      error_desc.append(error_value)\n      errorfile.append(fl)\n      errortimestamp.append(dateTime)\n      validationtype.append(\"checkdatatypelength\")\n  df_error = pd.DataFrame({'error_desc':error_desc,'error_file':errorfile,'error_time':errortimestamp,'validationtype':validationtype})\n  return df_error"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Scale and Precision validation","showTitle":true,"inputWidgets":{},"nuid":"bd05859f-aa20-47eb-8cb3-1e50b5ac90ab"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def datatype_validation(sdf,ddf,flname) :\n  column_names = ['error_desc', 'error_file', 'error_time','validationtype']\n  df_datatype_validation_errors = pd.DataFrame(columns = column_names)\n  #Check the count of records in the schema file data frame.\n  if (len(sdf)>0):\n    #Check the count of records in the data file data frame.\n    row_cnt = len(ddf)\n    # perform the validations only when the records exists\n    if (row_cnt > 0):\n          for index, row in sdf.iterrows():\n            col_index = row['indexno']\n            col_name = row['FieldName']\n            validation_function=row['validationfn']\n            sapdatatype=row['sapdatatype']\n            returnvalue=validate_datatype(col_index,validation_function,df_data_file,sapdatatype,flname,col_name)\n            df_datatype_validation_errors = df_datatype_validation_errors.append(returnvalue, ignore_index = True)\n            #print(returnvalue)\n    else: print('No records avilable in the data file dataframe for validation')\n  else: print('No records avilable in the scheme file dataframe for validation') \n  print(df_datatype_validation_errors)\n  if df_datatype_validation_errors.empty == False :\n    print(df_datatype_validation_errors)\n    df_datatype_validation_errors_spk = spark.createDataFrame(df_datatype_validation_errors.astype(str))\n    write_log(df_datatype_validation_errors_spk)\n  else: print(\"No Error Records to write\")  "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"DataType and Custom Validation","showTitle":true,"inputWidgets":{},"nuid":"8521c7ea-ed09-4965-8cb6-582f43374b94"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def deflt_date_time_cols(sdf,ddf):\n  df_schema = sdf\n  df_data = ddf\n  df_schema_filter=df_schema.query('sapdatatype == \"DATS\" | sapdatatype == \"TIMS\"')\n  for index, row in df_schema_filter.iterrows():\n    col_index = row['indexno'] #row_no\n    colid=str(col_index)\n    dtype = row['sapdatatype']\n    srccolname='df_data'+'['+colid+']'\n    eval_col_name=eval(srccolname)\n    tgtcolname=eval(colid)\n    if (dtype == 'DATS') :\n      df_data.loc[(eval_col_name == '00000000') | (eval_col_name == '') , tgtcolname] = \"19000101\"\n    else:\n      df_data.loc[(eval_col_name == '000000') | (eval_col_name == '') , tgtcolname] = \"000000\"\n  return(df_data) "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Default Date and Time values which are empty","showTitle":true,"inputWidgets":{},"nuid":"a8185b30-3781-487a-bdee-5ee9984ac174"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def remove_commas_from_numericalcols(sdf,ddf):\n  df_schema = sdf\n  df_data = ddf\n  df_schema_filter=df_schema.query('sapdatatype == \"INT\" | sapdatatype != \"000000\"')\n  for index, row in df_schema_filter.iterrows():\n    col_index = row['indexno'] #row_no\n    colid=col_index\n    df_data[colid] = df_data[colid].str.replace(',','')\n  return(df_data) "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Remove comma values from Int, Decimal and Float columns","showTitle":true,"inputWidgets":{},"nuid":"431ccfd1-ef4a-48f6-9652-4f0e6a33d4d5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def move_to_rejected(sfile,dpath):\n  df_read_rawfile=spark.read.format('csv').options(header='false',inferSchema='true').load(sfile)\n  df_read_rawfile.coalesce(1).write.mode('overwrite').format('com.databricks.spark.csv').option('header', 'false').save(dpath)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Move Rejected File to Rejected Folder Path","showTitle":true,"inputWidgets":{},"nuid":"9e640ef7-799e-4d64-b6e5-8bb13aa25ac8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#iterate the rows on Schema dataframe and assign the values and pick the values required for validation.\ndef datalength_validation(sdf,ddf,flname) : \n  column_names = ['error_desc', 'error_file', 'error_time','validationtype']\n  df_datalength_validation_errors = pd.DataFrame(columns = column_names)\n  if (len(sdf)>0):\n    row_cnt = len(ddf)\n    if (len(ddf)>0):\n        for index, row in sdf.iterrows():\n          col_index = row['indexno'] #row_no\n          dtlen = int(row['Length']) #permissible length\n          dtprec =int(row['decimals']) #permissible decimal length\n          dtype = row['sapdatatype']# sapdatatype\n          col_name = row['FieldName']#Fieldname\n          df_data_file = pd.DataFrame(ddf, columns = [col_index]) \n          # fn call to datalength validation function\n          returnvalue=precision_and_scale(col_index,dtlen,dtprec,dtype,df_data_file,flname,col_name)\n          #returnvalue=precision_and_scale(col_index,dtlen,dtprec,dtype,df_data_file,flname)\n          df_datalength_validation_errors = df_datalength_validation_errors.append(returnvalue, ignore_index = True)\n          df_datalength_validation_errors_fltr=df_datalength_validation_errors[df_datalength_validation_errors['error_desc']!='No Error']\n    else: print('No records avilable in the scheme file dataframe for validation') \n  else: print('No records avilable in the scheme file dataframe for validation') \n  if df_datalength_validation_errors_fltr.empty == False :\n    df_datalength_validation_errors_fltr_spk = spark.createDataFrame(df_datalength_validation_errors_fltr.astype(str))\n    write_log(df_datalength_validation_errors_fltr_spk)\n    #write_log_todb(df_datalength_validation_errors_fltr)\n  else: print(\"No Error Records to write\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Data Length Validation.","showTitle":true,"inputWidgets":{},"nuid":"a0f04006-b054-4d72-9ce5-8bf8c6986a75"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def write_process_status(flname,sf,st,enty,ft,zfp) :\n  #FileProcessing End Time\n  dateTime = datetime.now()\n  File_Name = flname #filename\n  Source = 'SAP' #sourcename\n  Loaded_By =\"DB-Materials-SAP-RawtoStageDataValidation\" #loadsource\n  Updated_By= \"DB-Materials-SAP-RawtoStageDataValidation\" #udpatedsource\n  Status_Flag = sf #statusflag:2-Qualifed,3-Rejected\n  Start_Time=st #file process start time\n  End_Time =dateTime #file process end time\n  Entity =enty #sap file entity name\n  File_Time =ft # derived from filename\n  Loaded_Time =dateTime # load time into log table\n  Updated_Time =dateTime # updated time into log table\n  Zone_FilePath=zfp # destination path of the file\n  DataDomain='Material'\n  # instantiate a empty spark dataframe\n  spark = SparkSession.builder.getOrCreate()\n  #create dummy columns with dummydata\n  columns = ['id','id1']\n  vals = [(0,0)]\n  # create audit dataframe\n  df_audit_frame = spark.createDataFrame(vals, columns)\n  #create audit columns and delete dummy columns\n  df_audit=df_audit_frame.withColumn(\"File_Name\", lit(File_Name)).withColumn(\"Source\", lit(Source)).withColumn(\"Status_Flag\", lit(Status_Flag)).withColumn(\"Start_Time\", lit(Start_Time)).withColumn(\"End_Time\", lit(End_Time)).withColumn(\"Entity\", lit(Entity)).withColumn(\"File_Time\", lit(File_Time)).withColumn(\"Loaded_By\", lit(Loaded_By)).withColumn(\"Loaded_Time\", lit(Loaded_Time)).withColumn(\"Updated_By\", lit(Updated_By)).withColumn(\"Updated_Time\", lit(Updated_Time)).withColumn(\"Zone_FilePath\", lit(Zone_FilePath)).withColumn(\"DataDomain\", lit(DataDomain)).drop(\"id\").drop(\"id1\")\n  df_audit_write = DataFrameWriter(df_audit) \n  #write audit record to database.\n  df_audit_write.jdbc(url=jdbcUrl, table= \"[dbo].[Log_DataFeedRepository]\", mode =\"append\", properties = connectionProperties)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Insert Processed Status in the Control Table.","showTitle":true,"inputWidgets":{},"nuid":"bb0015e5-6e3c-4537-8596-decce5682f40"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#list all the files from rawzone  \ndf_file_details=(dbutils.fs.ls(\"/mnt/rawzone\"))\ndf_cntrl_info_pd=pd.DataFrame(df_file_details)\n\nfor index, row in df_cntrl_info_pd.iterrows():\n  #fileprocessing starttime\n   # derive filename\n  flname=row['name'] \n  #filter the schema files and loop data files only\n  if (flname.startswith(\"S_\") != True ) :\n    st = datetime.now()\n    #derive schemaname\n    sflname = 'S_'+flname\n    #derive file data information\n    match_pattern = flname[-32:]\n    derive_filedate = match_pattern[:10].replace('-','')\n    #print(derive_filedate)\n    year = derive_filedate[:4]\n    month = derive_filedate[4:6]\n    day = derive_filedate[6:8]\n    extract_time = flname[-21:-13].replace('_',':')\n    #print(extract_time)\n    feednm=flname[:-33]\n    #print(feednm)\n    filetimestamp_str = year+'-'+month+'-'+day+'T'+extract_time+'.000000'\n    filetimestamp=datetime.strptime(filetimestamp_str, '%Y-%m-%dT%H:%M:%S.%f')\n    #print(filetimestamp_str)\n    #print(filetimestamp)\n    #set data file path\n    #datafilepath = raw_zone_mnt  + year + '/'+ month +'/'+ day +'/'+flname\n    datafilepath = raw_zone_mnt  + flname\n    #print(datafilepath)\n    # set schema file path\n    #schemafilepath = raw_zone_mnt  + year + '/'+ month +'/'+ day +'/'+sflname\n    schemafilepath = raw_zone_mnt  +sflname\n    #print(datafilepath)\n    #print(schemafilepath)\n    #set qualified folder path\n    qualifiedfolderpath = qffilepath+feednm\n    #rejected folder path\n    rejectedfolderpath = rjfilepath + year + '/'+ month +'/'+ day \n    # check if the schema file exists\n    #print(qualifiedfolderpath)\n    #print(rejectedfolderpath)\n    sfileexists = file_exists(schemafilepath)\n    # check if dataframe is empty\n    dfcheckempty= df_isempty(datafilepath)\n    # check if the data file exists\n    dfileexists = file_exists(datafilepath)\n    # process the files only when both schema file and data file exists\n    if (sfileexists == True) and (dfileexists == True) :\n      #check if dataframe is empty, this is possible when there are no incremental records coming in from the SAP Feeds.\n      if (dfcheckempty == False) :\n        # fn call to get a schema file data\n        #print(schemafilepath)\n        df_schema_file = generate_schemafile_df(schemafilepath)\n        # fn call to get the data file data\n        df_data_file_raw = generate_datafile_df(datafilepath)\n        #load data dataframe\n        df_data_file_dflt = deflt_date_time_cols(df_schema_file,df_data_file_raw)\n        #remove commas from numerical columns\n        df_data_file = remove_commas_from_numericalcols(df_schema_file,df_data_file_dflt)\n        #fn call to execute column count validation\n        duplicatecheckbusinesskey_validation_result=check_businesskeyduplicates(df_schema_file,df_data_file,flname)\n        # fn call to execute column count validation\n        columncountcheck_validation_result = column_count_check(df_schema_file,df_data_file,flname)\n        #fn call to execute datalength validation\n        data_length_validation_result = datalength_validation(df_schema_file,df_data_file,flname)\n        # fn call to execute datatype validation\n        data_type_validation_result = datatype_validation(df_schema_file,df_data_file,flname)\n        #df_datafile_final = merge_schemaanddatafiles(df_schema_file,df_data_file)\n        df_datafile_final = df_data_file\n        df_datafile_final['SeqNo']= df_datafile_final.index\n        qtflname = \"'\"+flname+\"'\"\n        #build the querystring to query the error tables\n        query = 'select error_desc,error_file,error_time,validationtype from {} where error_file = {}'.format(errorlogtable,qtflname)\n        querycnt = 'select count(*) as cnt from {} where error_file = {}'.format(errorlogtable,qtflname)\n        df_error = sqlContext.sql(querycnt).toPandas()\n        df_error_desc_result = sqlContext.sql(query)\n        df_error_desc_result_sql= df_error_desc_result.registerTempTable(\"df_error_desc_result\")\n        #pick only top 20 records for each file,validationtype error combination \n        df_error_desc=sqlContext.sql(\"select error_desc,error_file,error_time,validationtype from (select *,row_number() over (partition by error_file,validationtype order by error_time desc) as rn from df_error_desc_result) a where a.rn <=20\").toPandas()\n        #df_errcnt = int(df_error.get_value(0,'cnt')\n        df_errcnt = int(df_error.loc[0,'cnt'])\n        #print(df_errcnt)\n        #if(df_Material_filter.count() == 0)\n        #if (df_error.count()!=0) :\n        print(schemafilepath)\n        print(datafilepath)\n        print(rejectedfolderpath)\n        print(qualifiedfolderpath)\n        if (df_errcnt > 0 ):\n          print(\"entered this loop\")\n          if (file_exists(rejectedfolderpath) == True) :\n          #create the directory.\n            dbutils.fs.mv(schemafilepath,rejectedfolderpath)\n            dbutils.fs.mv(datafilepath,rejectedfolderpath) \n            sf=3\n            write_process_status(flname,sf,st,feednm,filetimestamp,rejectedfolderpath)\n            write_process_status(sflname,sf,st,feednm,filetimestamp,rejectedfolderpath)\n            write_log_todb(df_error_desc)\n          else:\n            dbutils.fs.mkdirs(rejectedfolderpath)\n            dbutils.fs.mv(schemafilepath,rejectedfolderpath)\n            dbutils.fs.mv(datafilepath,rejectedfolderpath)\n            sf=3\n            write_process_status(flname,sf,st,feednm,filetimestamp,rejectedfolderpath)\n            write_process_status(sflname,sf,st,feednm,filetimestamp,rejectedfolderpath)\n            write_log_todb(df_error_desc)\n        else :\n          if (file_exists(qualifiedfolderpath) == True):\n            print(\"writedata\")\n            #write_csv(df_datafile_final,qualifiedfolderpath,flname,filetimestamp)\n            dbutils.fs.mv(schemafilepath,qualifiedfolderpath)\n            dbutils.fs.mv(datafilepath,qualifiedfolderpath)\n            sf=2\n            write_process_status(flname,sf,st,feednm,filetimestamp,qualifiedfolderpath)\n            write_process_status(sflname,sf,st,feednm,filetimestamp,qualifiedfolderpath)\n          else :\n            #create the directory.\n            dbutils.fs.mkdirs(qualifiedfolderpath)\n            #write_csv(df_datafile_final,qualifiedfolderpath,flname,filetimestamp)\n            dbutils.fs.mv(schemafilepath,qualifiedfolderpath)\n            dbutils.fs.mv(datafilepath,qualifiedfolderpath)\n            sf=2\n            write_process_status(flname,sf,st,feednm,filetimestamp,qualifiedfolderpath)\n            write_process_status(sflname,sf,st,feednm,filetimestamp,qualifiedfolderpath)\n      else:\n        #the file is empty,therefore no data to process\n        print(\"nodata to write\")\n        sf=2\n        write_process_status(flname,sf,st,feednm,filetimestamp,qualifiedfolderpath)\n        write_process_status(sflname,sf,st,feednm,filetimestamp,qualifiedfolderpath)\n    else : \n      print(\"Schema file or data file is missing\")\n      #log when the schema file or the data file is missing.\n      error_desc='Schema file '+ sflname +' or ' +flname + ' is missing'\n      error_file = sflname + ' or ' + flname\n      validationtype = 'Fileexistsvalidation'\n      dateTime = datetime.now()\n      # if not already created automatically, instantiate Sparkcontext\n      spark = SparkSession.builder.getOrCreate()\n      column_names = ['error_desc', 'error_file', 'error_time','validationtype']\n      vals = [(error_desc,flname,str(dateTime),validationtype)]\n      error_df = spark.createDataFrame(vals, column_names)\n      #call the error log function\n      sf=3\n      st=dateTime \n      rejectedfolderpath ='NA'\n      write_process_status(flname,sf,st,feednm,filetimestamp,rejectedfolderpath)\n      write_log_todb(error_df.toPandas())\n  else :\n    print(\"No Files Found\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Main Function-Storage","showTitle":true,"inputWidgets":{},"nuid":"3e9b68c7-de87-421a-93e6-cf854c298c06"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Source Data Validation - Raw to Stage","dashboards":[],"language":"python","widgets":{},"notebookOrigID":3930682947574559}},"nbformat":4,"nbformat_minor":0}
